{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunReddy77/LLaMA_Finetuning/blob/main/LLaMA_3_2_1B_Instruct_Finetuning_on_the_IMBD_dataset_using_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eREbBiyV7bZv"
      },
      "source": [
        "# **Fine-Tuning Llama 3.2 1B for Sentiment Analysis**\n",
        "\n",
        "### **Objective:**\n",
        "This notebook documents a robust, end-to-end workflow for fine-tuning the `meta-llama/Llama-3.2-1B-Instruct` model on the IMDB Movie Reviews dataset. The primary goal is to demonstrate a professional methodology that balances performance with computational efficiency, rather than solely pursuing state-of-the-art metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### **Workflow at a Glance:**\n",
        "\n",
        "Our pipeline is structured to be reproducible, resource-aware, and methodologically sound.\n",
        "\n",
        "* üßπ **Data Integrity First:** We begin with a rigorous data cleaning and de-duplication phase to ensure the quality of our training data and prevent data leakage between splits.\n",
        "\n",
        "* üìä **Data-Driven Preprocessing:** We perform a token-level distribution analysis to make a justified, hardware-aware decision for our model's `max_length`. We then employ a stratified sampling strategy to create perfectly balanced train, validation, and test sets.\n",
        "\n",
        "* üéØ **Zero-Shot Baseline:** Before fine-tuning, we establish a quantitative baseline by evaluating the base model's zero-shot performance on the task. This allows us to precisely measure the value added by our adaptation.\n",
        "\n",
        "* üõ†Ô∏è **Efficient Fine-Tuning (QLoRA):** We use Quantized Low-Rank Adaptation (QLoRA) to efficiently fine-tune the model on a single T4 GPU, training only ~0.14% of the total parameters.\n",
        "\n",
        "* ü§ñ **Automated & Resilient Training:** The training process is automated with learning rate scheduling, frequent checkpointing to Google Drive, and early stopping based on validation loss to prevent overfitting and conserve compute.\n",
        "\n",
        "* üìà **Comprehensive Evaluation:** We conclude with a detailed evaluation of the best model checkpoint, analyzing not just the final metrics but also the training dynamics and error patterns via a confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1. Environment Setup**"
      ],
      "metadata": {
        "id": "3ndL1HF7FGjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLEASE CONNECT TO GPU RUNTIME BEFORE YOU PROCEED FURTHER"
      ],
      "metadata": {
        "id": "4qxeWl22kOd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installations**\n",
        "\n",
        "To ensure reproducibility and compatibility with the `meta-llama/Llama-3.2-1B-Instruct` model, we begin by upgrading the core Hugging Face libraries (`transformers`, `datasets`, `peft`, `accelerate`) and other essential packages to their latest versions. This step mitigates potential version-related conflicts within the Colab environment."
      ],
      "metadata": {
        "id": "GggfuYelBtIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX-uTu844Oss",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "print(\"‚è≥ Upgrading required libraries...\")\n",
        "!pip install -U -q transformers datasets peft accelerate bitsandbytes matplotlib scikit-learn wordcloud\n",
        "print(\"‚úÖ Libraries upgraded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f-DuJKf8OO0"
      },
      "source": [
        "### **Imports, Authentication, and Global Configuration**\n",
        "\n",
        "This cell handles the initial setup for the notebook. We import all necessary libraries, define global configuration variables like the `MODEL_NAME` for easy maintenance, and handle authentication.\n",
        "\n",
        "**First-Time Setup Instructions:**\n",
        "* **Hugging Face Token:** To access the gated Llama 3.2 model, you must provide a Hugging Face access token.\n",
        "    1.  Click the \"üîë\" icon in the left sidebar of Colab to open the **Secrets** manager.\n",
        "    2.  Create a new secret with the name `HF_TOKEN`.\n",
        "    3.  Paste your Hugging Face token (with at least \"read\" permissions) as the value.\n",
        "    4.  Ensure the \"Notebook access\" toggle is enabled.\n",
        "    5.  **Note:** The LLaMA 3.2 model is hosted in a gated repository on Hugging Face. You must request and receive access to the model before you can use it.\n",
        "\n",
        "* **Google Drive:** When the cell is run, you will be prompted to authorize access to your Google Drive. This is required for saving model checkpoints persistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH_FUeO74tbr"
      },
      "outputs": [],
      "source": [
        "# --- 2.1: Library Imports ---\n",
        "\n",
        "# Standard Python Libraries\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from glob import glob\n",
        "from textwrap import fill\n",
        "\n",
        "# Third-party Libraries for Data Handling and Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from IPython.display import display\n",
        "\n",
        "# Scikit-learn for metrics and feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# Hugging Face Libraries\n",
        "from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset\n",
        "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer,\n",
        "    BitsAndBytesConfig, DataCollatorWithPadding, EarlyStoppingCallback,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Colab-specific Libraries\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# --- 2.2: Hugging Face Authentication ---\n",
        "# We securely load the Hugging Face token from Colab's secrets manager.\n",
        "# This is the recommended practice to avoid exposing sensitive keys in the notebook.\n",
        "try:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "    print(\"‚úÖ Hugging Face token loaded successfully from Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Could not load Hugging Face token. Please ensure it is stored as a secret named 'HF_TOKEN'.\")\n",
        "    print(\"Falling back to notebook_login()...\")\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "\n",
        "\n",
        "# --- 2.3: Global Configuration ---\n",
        "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Mount Google Drive to save model checkpoints persistently.\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not mount Google Drive. Checkpoints will be saved locally. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvEuxcjB8wIa"
      },
      "source": [
        "## **Section 2: Exploratory Data Analysis and Data Preprocessing**\n",
        "\n",
        "Before proceeding to the modeling phase, a thorough Exploratory Data Analysis is essential to understand the dataset's structure, quality, and underlying patterns. This analysis will directly inform our data preprocessing and modeling strategies.\n",
        "\n",
        "Our EDA will cover five key steps:\n",
        "\n",
        "\n",
        "\n",
        "1.  **Data Loading & Initial Inspection:** Loading the dataset into a pandas DataFrame for easier manipulation.\n",
        "2.  **Data Cleaning:** Identifying and removing artifacts like HTML tags that could negatively impact model performance.\n",
        "3.  **Token Length Distribution:** Analyzing the length of reviews in tokens to make a data-driven decision for the `max_length` hyperparameter.\n",
        "4.  **Class Balance:** Checking the distribution of positive and negative labels to ensure the dataset is balanced and our evaluation metrics will be meaningful.\n",
        "5.  **N-gram and Word Cloud Analysis:** Visualizing the most common words and phrases to get a qualitative feel for the language associated with each sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Dataset"
      ],
      "metadata": {
        "id": "BgkLmtBKF_eR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKarG2aLy9Z"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "test_df = pd.DataFrame(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "quB9YYMSjhX5"
      },
      "outputs": [],
      "source": [
        "#@title Bug Fix\n",
        "\n",
        "# UNCOMMENT the below code and run this cell ONLY if the dataset in the above cell cannot be loaded due to corruption in Colab cache (You will see an error).\n",
        "\n",
        "# !rm -rf ~/.cache/huggingface/datasets\n",
        "# !rm -rf ~/.cache/huggingface/hub\n",
        "\n",
        "# !pip install -U datasets -q\n",
        "\n",
        "# RESTART THE SESSION - DO NOT INSTALL PACKAGES OR LIBRARIES AFTER RESTART"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us have a look at a sample review from the dataset"
      ],
      "metadata": {
        "id": "DP0y-FU7HFld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sample(1)['text'].values[0]"
      ],
      "metadata": {
        "id": "hq0aNB4BGP-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKVtm9MbIdYX"
      },
      "source": [
        "We see that the review contains html tags which are useless and  could be potentially misleading for our model to predict the correct sentiment. So, let us remove them using a simple regex function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing HTML Tags"
      ],
      "metadata": {
        "id": "XO4_suzLIjZB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aejK3FQ7IzcH"
      },
      "outputs": [],
      "source": [
        "def clean_html(text):\n",
        "    \"\"\"Removes all HTML tags from text using a regular expression.\"\"\"\n",
        "    return re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "print(\"Cleaning HTML tags from the dataset...\")\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean_html)\n",
        "test_df['text'] = test_df['text'].apply(clean_html)\n",
        "\n",
        "print(\"‚úÖ Cleaning complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLQQ9CgKxJi"
      },
      "source": [
        "### **Data Cleaning and De-duplication**\n",
        "\n",
        "The foundation of any reliable machine learning model is high-quality data. Raw datasets, especially those scraped from the web, often contain artifacts and inconsistencies that can degrade model performance and lead to misleading evaluation results. This section details the rigorous cleaning and de-duplication pipeline we implemented to ensure the integrity of our dataset.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Initial Quality Checks:** We first scan for and remove any `NaN` values or empty strings. This step prevents errors in the tokenization pipeline and ensures that every sample contains meaningful text.\n",
        "2.  **Intra-Set De-duplication:** We identify and remove any exact duplicate reviews *within* the training set and, separately, *within* the test set.\n",
        "3.  **Inter-Set De-duplication (Preventing Data Leakage):** This is the most critical cleaning step. We check for any reviews that exist in both the training and test sets. These are removed from the test set to guarantee that our final evaluation is performed on truly unseen data, providing an unbiased measure of the model's generalization ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5FMSsMXKjhr"
      },
      "outputs": [],
      "source": [
        "print(\"--- Starting Data De-duplication ---\")\n",
        "\n",
        "print(f\"Original training set size: {len(train_df)}\")\n",
        "print(f\"Original test set size: {len(test_df)}\")\n",
        "\n",
        "# --- Identify and remove duplicates within each set ---\n",
        "num_train_duplicates = train_df.duplicated(subset=['text']).sum()\n",
        "num_test_duplicates = test_df.duplicated(subset=['text']).sum()\n",
        "\n",
        "print(f\"\\nFound {num_train_duplicates} duplicates in the training set.\")\n",
        "print(f\"Found {num_test_duplicates} duplicates in the test set.\")\n",
        "\n",
        "train_df.drop_duplicates(subset=['text'], inplace=True, keep='first')\n",
        "test_df.drop_duplicates(subset=['text'], inplace=True, keep='first')\n",
        "\n",
        "print(f\"\\nSize of training set after internal de-duplication: {len(train_df)}\")\n",
        "print(f\"Size of test set after internal de-duplication: {len(test_df)}\")\n",
        "\n",
        "# --- Identify and remove duplicates across train and test sets ---\n",
        "# This is the most critical step to prevent data leakage\n",
        "train_texts = set(train_df['text'])\n",
        "test_in_train_mask = test_df['text'].isin(train_texts)\n",
        "num_cross_duplicates = test_in_train_mask.sum()\n",
        "\n",
        "print(f\"\\nFound {num_cross_duplicates} reviews in the test set that are also present in the training set.\")\n",
        "\n",
        "# Remove these cross-set duplicates from the test set\n",
        "test_df = test_df[~test_in_train_mask].copy()\n",
        "\n",
        "print(f\"\\nFinal size of test set after all de-duplication: {len(test_df)}\")\n",
        "\n",
        "# Check for and remove any missing values (NaNs)\n",
        "print(f\"\\nMissing values in train set: {train_df['text'].isnull().sum()}\")\n",
        "print(f\"Missing values in test set: {test_df['text'].isnull().sum()}\")\n",
        "train_df.dropna(subset=['text'], inplace=True)\n",
        "test_df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "# Check for and remove any empty strings that resulted from cleaning\n",
        "train_df['is_empty'] = train_df['text'].str.strip().eq('')\n",
        "test_df['is_empty'] = test_df['text'].str.strip().eq('')\n",
        "print(f\"\\nEmpty strings in train set after cleaning: {train_df['is_empty'].sum()}\")\n",
        "print(f\"Empty strings in test set after cleaning: {test_df['is_empty'].sum()}\")\n",
        "train_df = train_df[~train_df['is_empty']]\n",
        "test_df = test_df[~test_df['is_empty']]\n",
        "\n",
        "print(f\"\\nFinal size of train set after all cleaning: {len(train_df)}\")\n",
        "print(f\"\\nFinal size of test set after all cleaning: {len(test_df)}\")\n",
        "\n",
        "# --- Convert cleaned pandas DataFrames back to Hugging Face Datasets ---\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df),\n",
        "    'test': Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "print(\"\\n--- ‚úÖ De-duplication Complete. Using cleaned dataset for all further steps. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRzj1U5jNmT9"
      },
      "source": [
        "Since we are training an instruct fine-tuned model, it makes sense to augment the reviews with a prompt that instructs the model to classify the sentiment of the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CrYmas2Pj8-"
      },
      "outputs": [],
      "source": [
        "def create_prompt(review):\n",
        "    \"\"\"Formats the raw review text into a structured instruction prompt.\"\"\"\n",
        "    return f'''CLASSIFY the SENTIMENT of the following movie review and ANSWER IN ONLY ONE WORD - either 'POSITIVE' or 'NEGATIVE'.\\nReview: {review}\\nSentiment: '''\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(create_prompt)\n",
        "test_df['text'] = test_df['text'].apply(create_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rsMMNuZRALx"
      },
      "outputs": [],
      "source": [
        "print(train_df.sample(1)['text'].values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryiNysyDSjO0"
      },
      "source": [
        "### **Visualizing the Tokenization Process**\n",
        "\n",
        "To better understand how the model processes text, it's helpful to visualize the tokenization process for a single example. Tokenization is the step where raw text is converted into a sequence of numerical IDs that the model can understand. This process is fundamental to how transformers operate.\n",
        "\n",
        "We will take a random sample from our cleaned training data and visualize:\n",
        "1.  The raw text of the review.\n",
        "2.  The resulting list of numerical `input_ids` after tokenization.\n",
        "3.  The `input_ids` decoded back into their corresponding string tokens. This clearly shows how words, subwords, and punctuation are broken down into a vocabulary the model recognizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbYTZH2rSkq-"
      },
      "outputs": [],
      "source": [
        "# --- 1. Select a Sample Review ---\n",
        "sample_review = train_df.sample(1)['text'].values[0]\n",
        "\n",
        "\n",
        "# --- 1. Look at the Sample Review ---\n",
        "print(\"--- sample review before tokenization ---\")\n",
        "print(sample_review)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Tokenize the Text ---\n",
        "# We use the main tokenizer that will be used for training.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenized_output = tokenizer(sample_review)\n",
        "input_ids = tokenized_output['input_ids']\n",
        "\n",
        "\n",
        "# --- 4. Visualize the Results ---\n",
        "print(\"--- Tokenization Output ---\")\n",
        "print(f\"Total number of tokens: {len(input_ids)}\")\n",
        "print(\"\\nFirst 50 Input IDs (Numerical Representation):\")\n",
        "print(input_ids[:50])\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Decode the IDs back to token strings for visualization\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# Create a more readable visualization\n",
        "print(\"--- Decoded Tokens (How the Model 'Sees' the Text) ---\")\n",
        "# We'll display tokens in a wrapped line format for readability\n",
        "print(fill(\" | \".join(decoded_tokens), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwl8vt0Igd5S"
      },
      "source": [
        "### **Token Length Analysis**\n",
        "\n",
        "To make a data-driven decision for the `max_length` hyperparameter, we will now analyze the distribution of review lengths. Crucially, we will measure length in terms of **tokens** rather than words, as this is the actual input the model will process.\n",
        "\n",
        "**Methodology:**\n",
        "1.  We will calculate the number of tokens for each review in both our cleaned training and test DataFrames.\n",
        "2.  We will then plot the distributions side-by-side. This serves as a critical sanity check to ensure that the training and test sets have similar length characteristics, which validates that our test set is a fair representation for evaluation.\n",
        "3.  We will calculate key statistics like the mean and 75th percentile to guide our final `max_length` selection in the subsequent data preparation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mRSGxbugg9I"
      },
      "outputs": [],
      "source": [
        "print(\"--- Starting Tokenization Analysis ---\")\n",
        "\n",
        "# --- Calculate Token Lengths from DataFrames ---\n",
        "print(\"Calculating token lengths for train and test sets...\")\n",
        "# --- MODIFIED: Use the 'text' column as requested ---\n",
        "train_df['num_tokens'] = train_df['text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "test_df['num_tokens'] = test_df['text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "\n",
        "\n",
        "# --- Analyze Token Length Distribution ---\n",
        "# Calculate key statistics for both sets\n",
        "train_mean_len = train_df['num_tokens'].mean()\n",
        "train_p75_len = train_df['num_tokens'].quantile(0.75)\n",
        "test_mean_len = test_df['num_tokens'].mean()\n",
        "test_p75_len = test_df['num_tokens'].quantile(0.75)\n",
        "\n",
        "print(\"\\n--- Token Length Statistics ---\")\n",
        "print(f\"  Training Set  | Mean: {train_mean_len:.2f} | 75th Percentile: {train_p75_len:.2f}\")\n",
        "print(f\"  Test Set      | Mean: {test_mean_len:.2f} | 75th Percentile: {test_p75_len:.2f}\")\n",
        "\n",
        "\n",
        "# --- Plot distributions side-by-side ---\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
        "fig.suptitle('Review Length Distribution (in Tokens)', fontsize=18)\n",
        "\n",
        "# Plot for Training Set\n",
        "sns.histplot(data=train_df, x='num_tokens', bins=50, kde=True, color='teal', ax=ax1)\n",
        "ax1.axvline(train_mean_len, color='red', linestyle='--', label=f'Mean ({train_mean_len:.2f})')\n",
        "ax1.axvline(train_p75_len, color='blue', linestyle='--', label=f'75th Percentile ({train_p75_len:.2f})')\n",
        "ax1.set_title('Train Set', fontsize=16)\n",
        "ax1.set_xlabel('Token Count', fontsize=12)\n",
        "ax1.set_ylabel('Number of Samples', fontsize=12)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot for Test Set\n",
        "sns.histplot(data=test_df, x='num_tokens', bins=50, kde=True, color='salmon', ax=ax2)\n",
        "ax2.axvline(test_mean_len, color='red', linestyle='--', label=f'Mean ({test_mean_len:.2f})')\n",
        "ax2.axvline(test_p75_len, color='blue', linestyle='--', label=f'75th Percentile ({test_p75_len:.2f})')\n",
        "ax2.set_title('Test Set', fontsize=16)\n",
        "ax2.set_xlabel('Token Count', fontsize=12)\n",
        "ax2.set_ylabel('') # Hide y-axis label for cleaner look\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMDn88bXjdgG"
      },
      "source": [
        "### **Analysis and `max_length` Selection**\n",
        "\n",
        "**Observations:**\n",
        "The token length distributions for the training and test sets are nearly identical, which confirms that the test set is a representative sample for evaluation. The analysis shows a mean token length of approximately 320, with the 75th percentile around 380 tokens. This indicates that while there are long-tail outliers, the vast majority of reviews are well under 500 tokens.\n",
        "\n",
        "**Decision:**\n",
        "Based on this analysis, we will set a `max_length` of **384**. This value is a strategic choice for several reasons:\n",
        "1.  **Data Coverage:** It comfortably covers over 75% of the reviews in their entirety, ensuring the model sees complete context for most samples.\n",
        "2.  **Computational Feasibility:** It is well within the memory limits of a T4 GPU, allowing for a stable training process with a reasonable batch size.\n",
        "3.  **Efficiency:** It avoids the inefficiency of a much larger `max_length` (e.g., 512 or 1024), which would require adding a significant number of padding tokens to the majority of reviews, wasting computational resources.\n",
        "\n",
        "Furthermore, to ensure the model trains on the highest quality data, we will **filter out** reviews longer than 384 tokens rather than truncating them. This is a deliberate trade-off: we specialize our model on short-to-medium length reviews to guarantee that it never learns from incomplete information where the concluding sentiment might be cut off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhsbUSRuoET0"
      },
      "source": [
        "### **Final Dataset Curation via Stratified Sampling**\n",
        "\n",
        "With our EDA complete and a `max_length` of 384 chosen, we now construct the final datasets for our modeling pipeline. To ensure the highest data quality and prevent any potential biases from random sampling, we will employ a rigorous, multi-step stratified sampling approach.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Filter by Length:** We first filter the cleaned `train_df` and `test_df` to include only reviews with a token count less than or equal to our chosen `max_length` of 384. This ensures the model will only train on complete, un-truncated examples.\n",
        "2.  **Stratified Sampling:** To create perfectly balanced and non-overlapping datasets, we explicitly sample an equal number of positive and negative reviews from the filtered data pools. This guarantees a 50/50 class split in our final train, validation, and test sets, which is superior to simple random sampling.\n",
        "3.  **Final Splits:** We construct three distinct datasets: a 5,000-sample training set, a 500-sample validation set, and a 5,000-sample test set.\n",
        "4.  **Verification:** We conclude by visualizing the class balance of all three final datasets to confirm that our stratified sampling was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuSwqQJGkjtO"
      },
      "outputs": [],
      "source": [
        "# --- Filter DataFrames by Length ---\n",
        "max_length = 384\n",
        "print(f\"Filtering train and test sets for reviews <= {max_length} tokens...\")\n",
        "filtered_train_df = train_df[train_df['num_tokens'] <= max_length].copy()\n",
        "filtered_test_df = test_df[test_df['num_tokens'] <= max_length].copy()\n",
        "\n",
        "# Convert the filtered pandas DataFrames into Hugging Face Datasets\n",
        "filtered_train_ds = Dataset.from_pandas(filtered_train_df)\n",
        "filtered_test_ds = Dataset.from_pandas(filtered_test_df)\n",
        "\n",
        "print(\"‚úÖ Filtering complete.\")\n",
        "print(f\"  - Number of samples in filtered train set: {len(filtered_train_ds)}\")\n",
        "print(f\"  - Number of samples in filtered test set: {len(filtered_test_ds)}\")\n",
        "\n",
        "\n",
        "# --- Create Perfectly Balanced Datasets ---\n",
        "def create_balanced_set(dataset, num_samples_per_class, seed=42):\n",
        "    \"\"\"Samples an equal number of positive and negative reviews.\"\"\"\n",
        "    pos_set = dataset.filter(lambda ex: ex['label'] == 1)\n",
        "    neg_set = dataset.filter(lambda ex: ex['label'] == 0)\n",
        "\n",
        "    pos_sample = pos_set.shuffle(seed=seed).select(range(num_samples_per_class))\n",
        "    neg_sample = neg_set.shuffle(seed=seed).select(range(num_samples_per_class))\n",
        "\n",
        "    return concatenate_datasets([pos_sample, neg_sample]).shuffle(seed=seed)\n",
        "\n",
        "# Create a perfectly balanced training set of 5,000 samples\n",
        "final_train_set = create_balanced_set(filtered_train_ds, 2500)\n",
        "\n",
        "# To create a disjoint validation set, we first remove the training samples' text\n",
        "train_texts = set(final_train_set['text'])\n",
        "remaining_train_pool = filtered_train_ds.filter(lambda ex: ex['text'] not in train_texts)\n",
        "\n",
        "# Now create a perfectly balanced validation set from the remaining pool\n",
        "final_val_set = create_balanced_set(remaining_train_pool, 250)\n",
        "\n",
        "# Create the final test set from the filtered test data\n",
        "final_test_set = create_balanced_set(filtered_test_ds, 2500)\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Dataset Sizes ---\")\n",
        "print(f\"  - Final Training Set:   {len(final_train_set)} samples\")\n",
        "print(f\"  - Final Validation Set: {len(final_val_set)} samples\")\n",
        "print(f\"  - Final Test Set:       {len(final_test_set)} samples\")\n",
        "\n",
        "\n",
        "# --- Final Class Balance Check ---\n",
        "# Convert datasets to pandas DataFrames for easy plotting\n",
        "df_train_final = final_train_set.to_pandas()\n",
        "df_val_final = final_val_set.to_pandas()\n",
        "df_test_final = final_test_set.to_pandas()\n",
        "\n",
        "# Create a figure with three subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
        "fig.suptitle('Final Dataset Class Distributions (After Stratified Sampling)', fontsize=20)\n",
        "\n",
        "# Plot for Training Set\n",
        "sns.countplot(x='label', data=df_train_final, ax=ax1, palette='viridis', hue='label', legend=False)\n",
        "ax1.set_title(f'Train Set ({len(df_train_final)} samples)', fontsize=16)\n",
        "ax1.set_xlabel('Sentiment Label')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_xticks([0, 1])\n",
        "ax1.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "# Plot for Validation Set\n",
        "sns.countplot(x='label', data=df_val_final, ax=ax2, palette='plasma', hue='label', legend=False)\n",
        "ax2.set_title(f'Validation Set ({len(df_val_final)} samples)', fontsize=16)\n",
        "ax2.set_xlabel('Sentiment Label')\n",
        "ax2.set_ylabel('')\n",
        "ax2.set_xticks([0, 1])\n",
        "ax2.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "# Plot for Test Set\n",
        "sns.countplot(x='label', data=df_test_final, ax=ax3, palette='magma', hue='label', legend=False)\n",
        "ax3.set_title(f'Test Set ({len(df_test_final)} samples)', fontsize=16)\n",
        "ax3.set_xlabel('Sentiment Label')\n",
        "ax3.set_ylabel('')\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Print the exact counts for confirmation\n",
        "print(\"--- Label Counts ---\")\n",
        "print(f\"Train Set:\\n{df_train_final['label'].value_counts()}\\n\")\n",
        "print(f\"Validation Set:\\n{df_val_final['label'].value_counts()}\\n\")\n",
        "print(f\"Test Set:\\n{df_test_final['label'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PRbzj2_q3Ma"
      },
      "source": [
        "### **Final Distribution Sanity Check**\n",
        "\n",
        "As a final validation of our data preparation, we will visualize the token length distributions of our three final datasets: `train`, `validation`, and `test`. This confirms that our stratified sampling has produced sets with similar characteristics, which is crucial for trusting that the model's performance on the validation set will be indicative of its performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0MzrGTsq2m1"
      },
      "outputs": [],
      "source": [
        "# Create a figure with three subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6), sharey=True)\n",
        "fig.suptitle('Final Dataset Token Length Distributions', fontsize=20)\n",
        "\n",
        "# Plot for Training Set\n",
        "sns.histplot(data=df_train_final, x='num_tokens', bins=30, kde=True, color='royalblue', ax=ax1)\n",
        "ax1.set_title(f'Train Set ({len(df_train_final)} samples)', fontsize=16)\n",
        "ax1.set_xlabel('Token Count')\n",
        "ax1.set_ylabel('Number of Samples')\n",
        "ax1.set_xticks([0, 1])\n",
        "ax1.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "# Plot for Validation Set\n",
        "sns.histplot(data=df_val_final, x='num_tokens', bins=30, kde=True, color='seagreen', ax=ax2)\n",
        "ax2.set_title(f'Validation Set ({len(df_val_final)} samples)', fontsize=16)\n",
        "ax2.set_xlabel('Token Count')\n",
        "ax2.set_ylabel('')\n",
        "ax2.set_xticks([0, 1])\n",
        "ax2.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "# Plot for Test Set\n",
        "sns.histplot(data=df_test_final, x='num_tokens', bins=30, kde=True, color='salmon', ax=ax3)\n",
        "ax3.set_title(f'Test Set ({len(df_test_final)} samples)', fontsize=16)\n",
        "ax3.set_xlabel('Token Count')\n",
        "ax3.set_ylabel('')\n",
        "ax3.set_xticks([0, 1])\n",
        "ax3.set_xticklabels(['Negative', 'Positive'])\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- ‚úÖ Final Data Preparation and Validation Complete ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train and test distributions look similar. This is good."
      ],
      "metadata": {
        "id": "hBAm66TzSLej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are done with data preprocessing, we can drop the unncessary columns from our dataframes.\n"
      ],
      "metadata": {
        "id": "1bnwaU-MSkXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d-x3u7-l6JNy"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = ['is_empty', 'num_tokens', '__index_level_0__']\n",
        "\n",
        "df_train_final = df_train_final.drop(columns=cols_to_drop)\n",
        "df_val_final = df_val_final.drop(columns=cols_to_drop)\n",
        "df_test_final = df_test_final.drop(columns=cols_to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3: Zero-Shot Baseline Performance**\n",
        "\n",
        "Before committing to a full fine-tuning run, it's crucial to establish a performance baseline. This allows us to quantitatively measure the value added by our fine-tuning process.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Load Model and Helpers:** We will load the base `AutoModelForCausalLM` model and define our helper functions for creating prompts and generating predictions.\n",
        "2.  **Qualitative Sanity Check:** We will first run inference on a few random samples to get a qualitative feel for the model's zero-shot performance and ensure it is generating responses in the expected format. This validates our prompting strategy before we run a large-scale evaluation."
      ],
      "metadata": {
        "id": "vXgv89ElUELq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KABJgiZq5wTe"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Establishing Zero-Shot Baseline Performance ---\")\n",
        "\n",
        "# --- Load Base Model and Define Helper Functions ---\n",
        "# For this task, we need the model to generate text (the word \"positive\" or \"negative\"),\n",
        "# so we load it using AutoModelForCausalLM.\n",
        "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, # This global variable should be defined in a previous cell\n",
        "    torch_dtype=torch.bfloat16, # Use bfloat16 for faster inference\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer_inference = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer_inference.pad_token is None:\n",
        "    tokenizer_inference.pad_token = tokenizer_inference.eos_token\n",
        "\n",
        "def get_zero_shot_prediction(review_text):\n",
        "    \"\"\"Generates a sentiment prediction from the base model using an instruction prompt.\"\"\"\n",
        "    inputs = tokenizer_inference(review_text, return_tensors=\"pt\", truncation=True, max_length=384).to(\"cuda\")\n",
        "\n",
        "    # Pass the full `inputs` dictionary to the generate function\n",
        "    output_sequences = base_model_for_inference.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=15,\n",
        "        pad_token_id=tokenizer_inference.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer_inference.decode(output_sequences[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).lower()\n",
        "\n",
        "    if \"positive\" in generated_text: return 1\n",
        "    elif \"negative\" in generated_text: return 0\n",
        "    else: return -1\n",
        "\n",
        "# --- Qualitative Check on Random Samples ---\n",
        "# We run a few examples first as a sanity check before the full evaluation.\n",
        "print(\"\\n--- Qualitative Check on Random Samples ---\")\n",
        "num_samples_to_check = 5\n",
        "# We use the final_test_set created in the previous data prep stage\n",
        "random_indices = random.sample(range(len(final_test_set)), num_samples_to_check)\n",
        "\n",
        "for i in random_indices:\n",
        "    sample = final_test_set[i]\n",
        "    review_text = sample['text']\n",
        "    actual_label_id = sample['label']\n",
        "    actual_label = \"Positive\" if actual_label_id == 1 else \"Negative\"\n",
        "\n",
        "    predicted_label_id = get_zero_shot_prediction(review_text)\n",
        "    predicted_label = \"Unclear\"\n",
        "    if predicted_label_id == 1:\n",
        "        predicted_label = \"Positive\"\n",
        "    elif predicted_label_id == 0:\n",
        "        predicted_label = \"Negative\"\n",
        "\n",
        "    print(f\"Review: {review_text[:300]}...\")\n",
        "    print(f\"  -> Actual Label:    {actual_label}\")\n",
        "    print(f\"  -> Predicted Label: {predicted_label}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantitative Baseline Evaluation**\n",
        "\n",
        "After the qualitative sanity check, we proceed with a quantitative evaluation to get robust baseline metrics. We will run our `get_zero_shot_prediction` function on a 500-sample subset of our final, curated test set. This provides a statistically significant baseline score that we can directly compare against our fine-tuned model's performance on the same data subset later."
      ],
      "metadata": {
        "id": "OFrGrkiUUiu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUVJfsxEHES8"
      },
      "outputs": [],
      "source": [
        "# --- Quantitative Evaluation on the Full Test Set ---\n",
        "print(\"\\n--- Running Quantitative Evaluation on Full Test Set ---\")\n",
        "predictions, labels = [], []\n",
        "\n",
        "for example in tqdm(final_test_set.select(range(1000)), desc=\"Running Zero-Shot Inference\"):\n",
        "    prediction = get_zero_shot_prediction(example['text'])\n",
        "    if prediction != -1:\n",
        "        predictions.append(prediction)\n",
        "        labels.append(example['label'])\n",
        "\n",
        "# --- Report Baseline Metrics ---\n",
        "print(\"\\n--- Zero-Shot Baseline Results ---\")\n",
        "print(classification_report(labels, predictions, target_names=['negative', 'positive']))\n",
        "\n",
        "# --- Clean Up ---\n",
        "# It's critical to clear the memory before starting the fine-tuning process\n",
        "del base_model_for_inference\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n‚úÖ Baseline evaluation complete and memory cleared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uCjFAB7LDE-"
      },
      "source": [
        "## **Section 4: Final Tokenization & Model Loading**\n",
        "\n",
        "With our final, curated datasets prepared, the last step before training is to tokenize them. This section also handles loading the `AutoModelForSequenceClassification` model, which is specifically designed for this type of classification task.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Model Loading:** We will load the `Llama-3.2-1B-Instruct` model using the `AutoModelForSequenceClassification` class, which appends a trainable classification head. We will also apply our 4-bit quantization configuration to ensure memory efficiency.\n",
        "2.  **Tokenization:** We will apply our `tokenize_and_format` function to all three final datasets. This function wraps the text in our instructional prompt and then converts it into numerical IDs, padding and truncating to our chosen `max_length`.\n",
        "3.  **Final Formatting:** We will rename the label column and set the dataset format to PyTorch tensors, making them ready for the `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OJX0vxPk9dYk"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Initializing Data and Model for Fine-Tuning ---\")\n",
        "\n",
        "# --- Model and Tokenizer Loading ---\n",
        "# We now load the model using `AutoModelForSequenceClassification` to attach a\n",
        "# classification head. We also apply our 4-bit quantization configuration.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2, # Binary classification (positive/negative)\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# The tokenizer remains the same, but we must ensure it has a padding token.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "# --- Final Tokenization Pipeline ---\n",
        "# This function applies the prompt and tokenizes.\n",
        "def tokenize_and_format(example):\n",
        "    return tokenizer(example['text'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "# Apply the tokenization to our final, curated datasets\n",
        "print(\"\\nApplying final tokenization to train, validation, and test sets...\")\n",
        "# We list all columns to remove except for the label, which we need.\n",
        "columns_to_remove = ['text', 'label', 'is_empty', 'num_tokens', '__index_level_0__']\n",
        "\n",
        "train_dataset = final_train_set.map(\n",
        "    tokenize_and_format,\n",
        "    batched=False,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n",
        "\n",
        "eval_dataset = final_val_set.map(\n",
        "    tokenize_and_format,\n",
        "    batched=False,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n",
        "\n",
        "test_dataset = final_test_set.map(\n",
        "    tokenize_and_format,\n",
        "    batched=False,\n",
        "    remove_columns=columns_to_remove\n",
        ")\n",
        "\n",
        "# --- Final Formatting for PyTorch ---\n",
        "# It's safer to add the labels back after mapping if they get removed.\n",
        "train_dataset = train_dataset.add_column(\"labels\", final_train_set[\"label\"])\n",
        "eval_dataset = eval_dataset.add_column(\"labels\", final_val_set[\"label\"])\n",
        "test_dataset = test_dataset.add_column(\"labels\", final_test_set[\"label\"])\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "eval_dataset.set_format(\"torch\")\n",
        "test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"\\n--- ‚úÖ Tokenization Complete ---\")\n",
        "print(\"\\nFinal Tokenized Datasets Ready for Training:\")\n",
        "print(f\"  - Training Set Size:   {len(train_dataset)}\")\n",
        "print(f\"  - Validation Set Size: {len(eval_dataset)}\")\n",
        "print(f\"  - Test Set Size:       {len(test_dataset)}\")\n",
        "print(\"\\nSample of a final tokenized example:\")\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_rrh32o1aVr"
      },
      "source": [
        "## **Section 5: Fine-Tuning Strategy and Model Training**\n",
        "\n",
        "This section details the rationale behind our chosen hyperparameters and then executes the fine-tuning process. The strategy is designed to achieve robust performance while respecting the computational constraints of a single T4 GPU and demonstrating modern best practices.\n",
        "\n",
        "#### **Fine-Tuning Strategy Rationale**\n",
        "\n",
        "**Core Training Parameters (`TrainingArguments`)**\n",
        "\n",
        "* **Learning Rate (`learning_rate=2e-5`):**\n",
        "    * **Choice:** A learning rate of `2e-5` (0.00002) was selected. This is a standard, conservative starting point for fine-tuning transformer models with the AdamW optimizer.\n",
        "    * **Justification:** It is small enough to prevent the model from making drastic, destabilizing updates to its weights, yet large enough to allow for efficient learning. A lower rate would slow down training, while a higher rate could cause the model to fail to converge.\n",
        "    * **Alternatives Considered:** A slightly higher rate like `5e-5` could speed up initial learning but risks overshooting the optimal solution. A lower rate like `1e-5` would be safer but potentially much slower.\n",
        "\n",
        "* **Learning Rate Scheduler (`lr_scheduler_type=\"cosine\"`):**\n",
        "    * **Choice:** A cosine scheduler with a warmup period (`warmup_ratio=0.1`).\n",
        "    * **Justification:** This is a proven technique for stable training. The initial warmup period gradually increases the learning rate, preventing early instability. After the warmup, the rate smoothly decays in a cosine curve, allowing the model to settle into a good minimum in the loss landscape.\n",
        "    * **Alternatives Considered:** A linear scheduler is a simpler alternative but can sometimes decay the learning rate too aggressively at the end. A constant learning rate is generally not recommended as it makes it harder for the model to find a precise optimum.\n",
        "\n",
        "* **Batch Size (`per_device_train_batch_size=2`, `gradient_accumulation_steps=4`):**\n",
        "    * **Choice:** An effective batch size of 8 (2 * 4).\n",
        "    * **Justification:** The per-device batch size of 2 is the maximum that can reliably fit into the T4 GPU's memory with our chosen model and sequence length. To achieve a more stable gradient and improve model performance, we use gradient accumulation for 4 steps. This simulates a larger batch size without increasing memory requirements.\n",
        "    * **Alternatives Considered:** A larger per-device batch size would be ideal but is not feasible on the available hardware.\n",
        "\n",
        "**Parameter-Efficient Fine-Tuning (LoRA)**\n",
        "\n",
        "* **Rank (`r=16`):**\n",
        "    * **Choice:** A LoRA rank of 16.\n",
        "    * **Justification:** This provides a good balance between model capacity (the ability to learn the new task) and parameter efficiency. It allows the model to learn complex patterns without adding an excessive number of new weights, which would slow down training.\n",
        "    * **Alternatives Considered:** A lower rank (`r=8`) would be faster but might underfit. A higher rank (`r=32`) could offer more performance but at the cost of increased training time and memory, with a higher risk of overfitting the small dataset.\n",
        "\n",
        "* **Alpha (`lora_alpha=32`):**\n",
        "    * **Choice:** An alpha value of 32, which is double the rank.\n",
        "    * **Justification:** This is a common and effective heuristic. The scaling factor (`alpha / r`) becomes 2, which amplifies the influence of the LoRA adapters and acts as a specialized learning rate for the new weights, helping them learn more effectively.\n",
        "    * **Alternatives Considered:** Setting `alpha = r` (a scaling factor of 1) is a more conservative choice but might require a higher learning rate or more training time to achieve the same effect.\n",
        "\n",
        "**Regularization and Efficiency**\n",
        "\n",
        "* **Early Stopping (`EarlyStoppingCallback` with `patience=3`):**\n",
        "    * **Choice:** Stop training if the validation loss does not improve for 3 consecutive evaluation periods (3 * 250 = 750 steps).\n",
        "    * **Justification:** This is the most critical strategy for preventing overfitting and saving computational resources. It ensures that we stop training precisely when the model's ability to generalize to new data begins to degrade.\n",
        "    * **Alternatives Considered:** A lower patience might stop training too early, while a higher patience could waste time training an already-overfitting model.\n",
        "\n",
        "* **Checkpointing (`save_steps=250`, `load_best_model_at_end=True`):**\n",
        "    * **Choice:** Save a checkpoint every 250 steps and automatically load the best one at the end.\n",
        "    * **Justification:** Frequent saving protects against interruptions. Loading the best model ensures that our final evaluation is based on the model at its peak performance, not the overfit model from the final training step. This is a fundamental best practice for reliable model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUmfgGQn1hfd"
      },
      "outputs": [],
      "source": [
        "# --- LoRA and Model Preparation ---\n",
        "print(\"Preparing model for LoRA fine-tuning...\")\n",
        "model = prepare_model_for_kbit_training(\n",
        "    model,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "# --- Define Training Arguments and Trainer ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/llama3_imdb_finetune\",\n",
        "    save_strategy=\"steps\", save_steps=250, save_total_limit=4,\n",
        "    eval_strategy=\"steps\", eval_steps=250,\n",
        "    load_best_model_at_end=True, metric_for_best_model=\"loss\", greater_is_better=False,\n",
        "    learning_rate=2e-5, lr_scheduler_type=\"cosine\", warmup_ratio=0.1,\n",
        "    num_train_epochs=5, per_device_train_batch_size=2, gradient_accumulation_steps=4,\n",
        "    bf16=True, logging_steps=25, report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Helper function to compute metrics during evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(labels, predictions),\n",
        "        \"precision\": precision_score(labels, predictions),\n",
        "        \"recall\": recall_score(labels, predictions),\n",
        "    }\n",
        "\n",
        "class CustomDataCollator(DataCollatorWithPadding):\n",
        "    def __call__(self, features):\n",
        "        # Get the batch from the parent class\n",
        "        batch = super().__call__(features)\n",
        "\n",
        "        # The Trainer may add extra keys like 'num_items_in_batch'. We remove them\n",
        "        # to ensure compatibility with the model's forward pass.\n",
        "        # We get the expected arguments from the model's signature.\n",
        "        model_args = self.tokenizer.model_input_names\n",
        "\n",
        "        # Filter the batch to only include keys the model expects\n",
        "        filtered_batch = {k: v for k, v in batch.items() if k in model_args}\n",
        "\n",
        "        # We must manually add the 'labels' back if they exist in the original batch\n",
        "        if 'labels' in batch:\n",
        "            filtered_batch['labels'] = batch['labels']\n",
        "\n",
        "        return filtered_batch\n",
        "\n",
        "# Initialize the Trainer with our custom data collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=CustomDataCollator(tokenizer=tokenizer), # Use the custom collator\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "\n",
        "# --- Execute Training ---\n",
        "print(\"\\nüöÄ Starting robust fine-tuning workflow...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"üõë Training interrupted manually. Proceeding to final evaluation with the best model found so far.\")\n",
        "\n",
        "print(\"\\nüéâ Training process complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTh-d9zX-TXl"
      },
      "source": [
        "## **Section 6: Final Evaluation and Analysis**\n",
        "\n",
        "With the training process complete, we now perform a final, comprehensive evaluation on the held-out test set. To ensure a clean environment and to demonstrate how to use the saved model, this section is designed to be run after restarting the notebook's runtime.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Load Best Model:** We begin by loading the base `Llama-3.2-1B-Instruct` model and then applying our saved LoRA adapter from the best checkpoint (`checkpoint-250`). This ensures we are evaluating the model at its peak performance.\n",
        "2.  **Single Evaluation Pass:** To work efficiently, we will run `trainer.predict()` **once** on a 500-sample subset of the test set. This single call returns an object containing all the necessary information: the final metrics, the raw predictions, and the true labels.\n",
        "3.  **Quantitative Analysis:** We will display the key performance metrics (Loss, Accuracy, F1 Score, etc.) in a clean, formatted table.\n",
        "4.  **Training Dynamics Visualization:** We will load the `trainer_state.json` from the latest checkpoint to reconstruct and plot the training and validation loss curves. This visualization is crucial for diagnosing the training process and confirming that our early stopping strategy was effective.\n",
        "5.  **Error Analysis:** Finally, we will use the predictions from our evaluation pass to generate a confusion matrix, allowing for a detailed analysis of the model's error patterns."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load the Best Model from Checkpoint ---\n",
        "print(\"\\nLoading best model from checkpoint...\")\n",
        "best_model_path = \"/content/drive/MyDrive/llama3_imdb_finetune/checkpoint-250\"\n",
        "\n",
        "# Load the base model with quantization\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, best_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# --- Initialize Trainer for Evaluation ---\n",
        "# We only need the trainer for its `.predict()` method.\n",
        "final_trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nüî• Evaluating the best model on the filtered test set...\")\n",
        "test_results = final_trainer.predict(test_dataset.select(range(1000)))\n",
        "\n",
        "\n",
        "# --- Display Metrics in a Clean Table ---\n",
        "# We extract the metrics directly from the results object.\n",
        "final_metrics = test_results.metrics\n",
        "metrics_df = pd.DataFrame([final_metrics])\n",
        "metrics_df = metrics_df.rename(columns={\n",
        "    \"test_loss\": \"Loss\",\n",
        "    \"test_accuracy\": \"Accuracy\",\n",
        "    \"test_f1\": \"F1 Score\",\n",
        "    \"test_precision\": \"Precision\",\n",
        "    \"test_recall\": \"Recall\",\n",
        "    \"test_runtime\": \"Runtime (s)\",\n",
        "})\n",
        "# Select only the columns we want to display\n",
        "display_metrics = metrics_df[['Loss', 'Accuracy', 'F1 Score', 'Precision', 'Recall', 'Runtime (s)']]\n",
        "\n",
        "print(\"\\n--- Final Model Performance on Test Set (from Checkpoint-250) ---\")\n",
        "display(display_metrics.round(4).T)\n",
        "\n",
        "\n",
        "# --- Visualize Training and Validation Loss ---\n",
        "print(\"\\nLoading training history to plot loss curves...\")\n",
        "output_dir = \"/content/drive/MyDrive/llama3_imdb_finetune\"\n",
        "checkpoint_dirs = sorted(glob(f\"{output_dir}/checkpoint-*\"), key=lambda d: int(d.split('-')[-1]))\n",
        "log_history = []\n",
        "if checkpoint_dirs:\n",
        "    latest_checkpoint = max(checkpoint_dirs, key=lambda d: int(d.split('-')[-1]))\n",
        "    trainer_state_path = os.path.join(latest_checkpoint, \"trainer_state.json\")\n",
        "    if os.path.exists(trainer_state_path):\n",
        "        with open(trainer_state_path, 'r') as f:\n",
        "            log_history = json.load(f)['log_history']\n",
        "    else:\n",
        "        print(f\"Warning: trainer_state.json not found in {latest_checkpoint}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No training history found in checkpoints.\")\n",
        "\n",
        "train_logs = [log for log in log_history if 'loss' in log]\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "train_df = pd.DataFrame(train_logs)\n",
        "eval_df = pd.DataFrame(eval_logs)\n",
        "\n",
        "if not train_df.empty and not eval_df.empty:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_df['step'], train_df['loss'], label='Training Loss', color='royalblue')\n",
        "    plt.plot(eval_df['step'], eval_df['eval_loss'], label='Validation Loss', color='darkorange', marker='o')\n",
        "    plt.title('Training and Validation Loss Curves', fontsize=16)\n",
        "    plt.xlabel('Training Steps', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    best_step = eval_df.loc[eval_df['eval_loss'].idxmin()]['step']\n",
        "    plt.axvline(x=best_step, color='red', linestyle='--', label=f'Best Model (Step {int(best_step)})')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Could not generate loss plot due to missing log history.\")\n",
        "\n",
        "\n",
        "# --- Generate Confusion Matrix ---\n",
        "# We use the predictions and labels from the results object we already have.\n",
        "print(\"\\nGenerating confusion matrix...\")\n",
        "y_pred = np.argmax(test_results.predictions, axis=1)\n",
        "y_true = test_results.label_ids\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation and analysis complete.\")"
      ],
      "metadata": {
        "id": "jclSdwy_vTGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis of Evaluation Results**\n",
        "\n",
        "The evaluation of our fine-tuned model demonstrates a significant improvement over the zero-shot baseline and confirms the effectiveness of our training strategy.\n",
        "\n",
        "**Quantitative Performance:**\n",
        "The model achieved an outstanding **F1 Score of 0.9421** and an **Accuracy of 0.9420** on the held-out test set. This indicates a very high degree of correctness in its predictions. A detailed look at Precision (0.9291) and Recall (0.9555) reveals a slight tendency for the model to have a higher recall. This means it is exceptionally good at identifying all the actual positive reviews, even if it means occasionally misclassifying a negative review as positive (a false positive).\n",
        "\n",
        "**Training Dynamics (Loss Curve Analysis):**\n",
        "The loss curve visualization is critical. It shows a healthy, decreasing training loss, confirming that the model was learning effectively. More importantly, it validates our early stopping strategy by clearly showing that the validation loss reached its minimum at **step 250** before starting to increase, a classic sign of overfitting. Our use of `load_best_model_at_end=True` ensured that the model we evaluated here is the one from that peak performance checkpoint, not a later, overfitted version.\n",
        "\n",
        "**Error Analysis (Confusion Matrix):**\n",
        "The confusion matrix provides a granular view of the model's predictions.\n",
        "* **High True Positives and True Negatives:** The high values on the main diagonal (236 TP, 235 TN) show that the model is highly accurate for both classes.\n",
        "* **Error Asymmetry:** The model made slightly more False Positive errors (18) than False Negative errors (11). This aligns perfectly with the precision and recall scores, confirming that the model is more likely to incorrectly label a negative review as positive than it is to miss an actual positive review.\n",
        "\n",
        "**Overall Conclusion:**\n",
        "The fine-tuning process was highly successful. The final model is a robust and accurate sentiment classifier, and the training workflow correctly identified the optimal model while preventing overfitting."
      ],
      "metadata": {
        "id": "vrwzpFPLfJoK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O3yCc5p-rUW"
      },
      "source": [
        "## **Section 7: Inference Examples and Final Model Save**\n",
        "\n",
        "The final step in our workflow is to perform a qualitative analysis of our fine-tuned model. While quantitative metrics provide a summary of performance, testing the model on specific examples gives us a more intuitive feel for its capabilities and limitations.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Curated Examples:** We will first test the model on a few hand-crafted reviews designed to probe its understanding of clearly positive, clearly negative, and more nuanced or sarcastic language. This helps assess its robustness.\n",
        "2.  **Side-by-Side Comparison:** To fulfill the assignment's requirement, we will then run inference on several random, unseen samples from the original test set and display the results in a table, comparing the model's predicted label directly against the actual label.\n",
        "3.  **Final Model Save:** After all evaluations are complete, we will save the final, best-performing LoRA adapter and its tokenizer to Google Drive. This creates a portable artifact that can be easily loaded for future use without needing to retrain the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZeTNYiI5c9j"
      },
      "outputs": [],
      "source": [
        "# Define a prediction function that encapsulates the full pipeline\n",
        "def predict_sentiment(text: str):\n",
        "    \"\"\"\n",
        "    Takes raw text, applies the full preprocessing pipeline (cleaning, prompting,\n",
        "    tokenizing), and returns the model's predicted sentiment as a string.\n",
        "    \"\"\"\n",
        "    # The create_prompt and clean_html functions should be available from previous cells\n",
        "    cleaned_text = clean_html(text)\n",
        "    prompt = create_prompt(cleaned_text)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=384).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "    return \"Positive\" if predicted_class_id == 1 else \"Negative\"\n",
        "\n",
        "# --- Test on Curated Examples ---\n",
        "print(\"\\n--- Testing on Curated Examples ---\")\n",
        "positive_review = \"This was one of the best movies I have ever seen! The acting was incredible, the story was moving, and the cinematography was breathtaking. I would recommend this to everyone.\"\n",
        "print(f\"Review: {positive_review}\")\n",
        "print(f\"Predicted Sentiment: {predict_sentiment(positive_review)}\\n\")\n",
        "\n",
        "negative_review = \"A complete waste of time. The plot was nonsensical, the characters were flat, and I was bored from the first minute. I can't believe this got made.\"\n",
        "print(f\"Review: {negative_review}\")\n",
        "print(f\"Predicted Sentiment: {predict_sentiment(negative_review)}\\n\")\n",
        "\n",
        "nuanced_review = \"Wow, what a 'masterpiece'. I'm sure the director thought the endless, shaky camera shots were 'artistic'. And the dialogue? 'Brilliant'. I've definitely never heard such profound lines as 'hey' and 'what's up' before. A must-see if you enjoy staring at a blank screen for two hours.\"\n",
        "print(f\"Review: {nuanced_review}\")\n",
        "print(f\"Predicted Sentiment: {predict_sentiment(nuanced_review)}\\n\")\n",
        "\n",
        "\n",
        "# --- Side-by-Side Comparison on Unseen Test Data ---\n",
        "print(\"\\n--- Side-by-Side Comparison on Unseen Test Data ---\")\n",
        "# We select a few random examples from the original, unfiltered test set to see\n",
        "# how the model performs on real-world data.\n",
        "original_test_set = load_dataset(\"imdb\", split='test')\n",
        "num_samples = 5\n",
        "random_indices = random.sample(range(len(original_test_set)), num_samples)\n",
        "\n",
        "results = []\n",
        "for i in random_indices:\n",
        "    sample = original_test_set[i]\n",
        "    review_text = sample['text']\n",
        "    actual_label = \"Positive\" if sample['label'] == 1 else \"Negative\"\n",
        "    predicted_label = predict_sentiment(review_text)\n",
        "\n",
        "    results.append({\n",
        "        \"Review Snippet\": clean_html(review_text)[:400] + \"...\",\n",
        "        \"Actual Label\": actual_label,\n",
        "        \"Predicted Label\": predicted_label\n",
        "    })\n",
        "\n",
        "# Display results in a clean DataFrame for easy comparison\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)\n",
        "\n",
        "\n",
        "# --- Save Final Model Artifact ---\n",
        "# Finally, we save the LoRA adapter of our best-performing model to a clean directory.\n",
        "final_model_path = \"/content/drive/MyDrive/llama3_imdb_finetune/final_best_model\"\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"\\n‚úÖ Final best model adapter saved to {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 8: Final Analysis, Conclusion, and Future Work**\n",
        "\n",
        "### **Analysis of Inference Results**\n",
        "\n",
        "The final inference tests provide valuable qualitative insights that complement our quantitative metrics. The model demonstrates high accuracy on both the curated, straightforward examples and the random, unseen test samples. This confirms that the fine-tuning process was highly effective in teaching the model to recognize common positive and negative sentiment patterns.\n",
        "\n",
        "The most insightful result, however, is the model's misclassification of the sarcastic review. It correctly identified the positive valence of surface-level words like `'masterpiece'` and `'brilliant'` but failed to grasp the overwhelmingly negative context provided by the surrounding phrases (\"*I'm sure the director thought...*\", \"*A must-see if you enjoy staring at a blank screen...*\").\n",
        "\n",
        "This \"intelligent failure\" is a critical finding. It clearly defines the boundary of the current model's capabilities, showing that it has mastered the primary task but has not yet learned to reliably detect higher-order linguistic phenomena like sarcasm. This provides a clear direction for future improvement.\n",
        "\n",
        "### **Concluding Remarks**\n",
        "\n",
        "This project successfully demonstrated a complete, end-to-end workflow for fine-tuning the `meta-llama/Llama-3.2-1B-Instruct` model for sentiment analysis. Through a rigorous and well-documented process, we achieved the following:\n",
        "\n",
        "1.  **Established a Robust Pipeline:** We implemented a professional workflow that included data cleaning, de-duplication, stratified sampling, and a data-driven approach to hyperparameter selection.\n",
        "2.  **Quantified Improvement:** We established a quantitative zero-shot baseline and proved that our fine-tuning process resulted in a significant performance uplift.\n",
        "3.  **Achieved High Performance:** The final model achieved an F1 score of over **0.94**, indicating a high degree of accuracy and reliability on the target task.\n",
        "4.  **Identified Model Limitations:** Through qualitative analysis, we identified a key area for future improvement: the model's handling of complex nuances like sarcasm.\n",
        "\n",
        "The final result is a highly effective sentiment classifier, specialized for the IMDB dataset, created through a process that was efficient, reproducible, and methodologically sound.\n",
        "\n",
        "### **Future Work**\n",
        "\n",
        "The final model is highly performant on the subset of data it was trained on. To create an even more robust and generalized model, future work could explore the following paths:\n",
        "\n",
        "1.  **Scaling Up Data:** The most impactful next step would be to re-run this same robust workflow on the full, filtered training dataset, which would likely improve performance on a wider variety of reviews.\n",
        "2.  **Training on Longer Sequences:** With access to more powerful hardware (e.g., an A100 GPU), the `max_length` could be increased to 512 or higher to train a model that is an expert on longer, more complex reviews.\n",
        "3.  **Targeted Data Augmentation:** To address the model's weakness with sarcasm, a targeted dataset could be created or sourced containing examples of sarcastic and nuanced reviews to further fine-tune the model on this specific challenge.\n",
        "4.  **Hyperparameter Tuning:** A systematic hyperparameter search (e.g., for LoRA rank `r` or learning rate) could potentially yield further marginal gains in performance."
      ],
      "metadata": {
        "id": "S_LTSjsAgusB"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}